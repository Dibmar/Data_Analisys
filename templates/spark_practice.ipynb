{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0fa281aee82f4477ae52abf752de28cd069eade5886616c2da4ed23d36079b5d2",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa281aee82f4477ae52abf752de28cd069eade5886616c2da4ed23d36079b5d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spark practice\n",
    "The following set of exercises has been sourced from: https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565\n",
    "\n",
    "## Exercise 1\n",
    "Find out how many orders, how many products and how many sellers are in the data. How many products have been sold at least once? Which is the product contained in more orders?\n",
    "\n",
    "## Exercise 2\n",
    "How many distinct products have been sold in each day?\n",
    "\n",
    "## Excercise 3\n",
    "What is the average revenue of the orders?\n",
    "\n",
    "## Excercise 4\n",
    "For each seller, what is the average % contribution of an order to the seller's daily quota?\n",
    "\n",
    "## Exercise 5\n",
    "Who are the second most selling and the least selling persons (sellers) for each product? Who are those for product with `product_id = 0`\n",
    "\n",
    "## Excercise 6\n",
    "Create a new column called \"hashed_bill\" defined as follows:\n",
    "\n",
    "    - if the order_id is even: apply MD5 hashing iteratively to the bill_raw_text field, once for each 'A' (capital 'A') present in the text. E.g. if the bill text is 'nbAAnllA', you would apply hashing three times iteratively (only if the order number is even)\n",
    "    - if the order_id is odd: apply SHA256 hashing to the bill text\n",
    "\n",
    "Finally, check if there are any duplicate on the new column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install pyspark-stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('pyspark_practice')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "\n"
   ]
  }
 ]
}